{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47affe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PMLS\\OneDrive\\Desktop\\IMDB Semantic Analysis\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2696e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ae6c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a122416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dcc51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"goemotions.csv\")\n",
    "df = df.rename(columns={\"id\": \"comment_id\"})\n",
    "df = df[df[\"example_very_unclear\"] == False]\n",
    "\n",
    "# **Add this line to limit training size**\n",
    "df = df.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c54d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text comment_id  \\\n",
      "0                                       That game hurt.    eew5j0j   \n",
      "2        You do right, if you don't care then fuck 'em!    ed2mah1   \n",
      "3                                    Man I love reddit.    eeibobj   \n",
      "4     [NAME] was nowhere near them, he was by the Fa...    eda6yn6   \n",
      "5     Right? Considering itâ€™s such an important docu...    eespn2i   \n",
      "...                                                 ...        ...   \n",
      "1009  Probably overdose in a hotel after losing my v...    eeg7349   \n",
      "1010  You're such a good troll. Good job! Way to rea...    ee3gfo0   \n",
      "1011                                 Glad you liked it!    eep717n   \n",
      "1012                                          GG haha ðŸ˜‰    edn2vyv   \n",
      "1013  I have a boobs but a penis, itâ€™s quite the emb...    eczunpk   \n",
      "\n",
      "                   author            subreddit    link_id   parent_id  \\\n",
      "0                   Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "2                Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "3           MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "4     American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "5            ImperialBoss           TrueReddit  t3_aizyuz  t1_eesoak0   \n",
      "...                   ...                  ...        ...         ...   \n",
      "1009        HowardAdderly    IncelsWithoutHate  t3_ahdsps  t1_eefy3w7   \n",
      "1010             st1nc1ty      MurderedByWords  t3_ag051c  t1_ee3d7h6   \n",
      "1011          Scottiedogg            Parenting  t3_aimfta  t1_eep6453   \n",
      "1012          WhiteYamagi         DeadBedrooms  t3_ae23x4   t3_ae23x4   \n",
      "1013           rupertofly             AskWomen  t3_abdnsl   t3_abdnsl   \n",
      "\n",
      "       created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0     1.548381e+09         1                 False           0  ...     0   \n",
      "2     1.546428e+09        37                 False           0  ...     0   \n",
      "3     1.547965e+09        18                 False           0  ...     1   \n",
      "4     1.546669e+09         2                 False           0  ...     0   \n",
      "5     1.548280e+09        61                 False           0  ...     0   \n",
      "...            ...       ...                   ...         ...  ...   ...   \n",
      "1009  1.547916e+09        61                 False           0  ...     0   \n",
      "1010  1.547522e+09        27                 False           1  ...     0   \n",
      "1011  1.548174e+09        62                 False           0  ...     0   \n",
      "1012  1.547049e+09        72                 False           0  ...     0   \n",
      "1013  1.546320e+09        60                 False           0  ...     0   \n",
      "\n",
      "      nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0               0         0      0            0       0        0        1   \n",
      "2               0         0      0            0       0        0        0   \n",
      "3               0         0      0            0       0        0        0   \n",
      "4               0         0      0            0       0        0        0   \n",
      "5               0         0      0            0       0        0        0   \n",
      "...           ...       ...    ...          ...     ...      ...      ...   \n",
      "1009            0         1      0            0       0        0        0   \n",
      "1010            0         0      1            0       0        0        0   \n",
      "1011            0         0      0            0       0        0        0   \n",
      "1012            0         0      0            0       0        0        0   \n",
      "1013            0         0      0            0       0        0        0   \n",
      "\n",
      "      surprise  neutral  \n",
      "0            0        0  \n",
      "2            0        1  \n",
      "3            0        0  \n",
      "4            0        1  \n",
      "5            0        0  \n",
      "...        ...      ...  \n",
      "1009         0        0  \n",
      "1010         0        0  \n",
      "1011         0        0  \n",
      "1012         0        0  \n",
      "1013         0        0  \n",
      "\n",
      "[1000 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1508eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate simplified 'labels' as list\n",
    "emotion_cols = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\",\n",
    "    \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\",\n",
    "    \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\",\n",
    "    \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
    "]\n",
    "\n",
    "def make_labels(row):\n",
    "    return [emo for emo in emotion_cols if row[emo] == 1]\n",
    "\n",
    "df[\"labels\"] = df.apply(make_labels, axis=1)\n",
    "\n",
    "# Remove rows where 'labels' contain \"neutral\"\n",
    "df = df[~df[\"labels\"].apply(lambda x: \"neutral\" in x)]\n",
    "\n",
    "# Optional sentiment column (positive/negative)\n",
    "pos = {\"admiration\", \"amusement\", \"approval\", \"caring\", \"desire\", \"excitement\", \"gratitude\", \"joy\", \"love\", \"optimism\", \"pride\", \"relief\"}\n",
    "neg = {\"anger\", \"annoyance\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"fear\", \"grief\", \"nervousness\", \"remorse\", \"sadness\"}\n",
    "\n",
    "def map_sentiment(lbls):\n",
    "    return \"positive\" if set(lbls) & pos else \"negative\"\n",
    "\n",
    "df[\"sentiment\"] = df[\"labels\"].apply(map_sentiment)\n",
    "\n",
    "# Drop 'comment_id' and keep only the necessary columns\n",
    "df = df[[\"text\", \"example_very_unclear\", \"labels\", \"sentiment\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77f8973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  example_very_unclear  \\\n",
      "0                                       That game hurt.                 False   \n",
      "3                                    Man I love reddit.                 False   \n",
      "5     Right? Considering itâ€™s such an important docu...                 False   \n",
      "6     He isn't as big, but he's still quite popular....                 False   \n",
      "7     That's crazy; I went to a super [RELIGION] hig...                 False   \n",
      "...                                                 ...                   ...   \n",
      "1009  Probably overdose in a hotel after losing my v...                 False   \n",
      "1010  You're such a good troll. Good job! Way to rea...                 False   \n",
      "1011                                 Glad you liked it!                 False   \n",
      "1012                                          GG haha ðŸ˜‰                 False   \n",
      "1013  I have a boobs but a penis, itâ€™s quite the emb...                 False   \n",
      "\n",
      "                   labels sentiment  \n",
      "0               [sadness]  negative  \n",
      "3                  [love]  positive  \n",
      "5             [gratitude]  positive  \n",
      "6           [disapproval]  negative  \n",
      "7             [amusement]  positive  \n",
      "...                   ...       ...  \n",
      "1009   [desire, optimism]  positive  \n",
      "1010  [admiration, pride]  positive  \n",
      "1011          [gratitude]  positive  \n",
      "1012                [joy]  positive  \n",
      "1013      [embarrassment]  negative  \n",
      "\n",
      "[752 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8358950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\", \"sentiment\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f79335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text sentiment\n",
      "0                                       That game hurt.  negative\n",
      "3                                    Man I love reddit.  positive\n",
      "5     Right? Considering itâ€™s such an important docu...  positive\n",
      "6     He isn't as big, but he's still quite popular....  negative\n",
      "7     That's crazy; I went to a super [RELIGION] hig...  positive\n",
      "...                                                 ...       ...\n",
      "1009  Probably overdose in a hotel after losing my v...  positive\n",
      "1010  You're such a good troll. Good job! Way to rea...  positive\n",
      "1011                                 Glad you liked it!  positive\n",
      "1012                                          GG haha ðŸ˜‰  positive\n",
      "1013  I have a boobs but a penis, itâ€™s quite the emb...  negative\n",
      "\n",
      "[752 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93b0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956be855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses a single text string.\n",
    "    1. Removes HTML tags\n",
    "    2. Lowercases text\n",
    "    3. Removes punctuation and numbers\n",
    "    4. Tokenizes text\n",
    "    5. Removes stop words\n",
    "    6. Lemmatizes words\n",
    "    \"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization is often better than stemming\n",
    "    text = ' '.join(tokens)\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6553ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing all reviews .....\n",
      "Preprcessing Complete!\n",
      "\n",
      "Comparing Orignal vs Cleaned Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>game hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>man love reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Right? Considering itâ€™s such an important docu...</td>\n",
       "      <td>right consider important document know damned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>He isn't as big, but he's still quite popular....</td>\n",
       "      <td>be not big he s still quite popular I ve hear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>That's crazy; I went to a super [RELIGION] hig...</td>\n",
       "      <td>that s crazy go super religion high school thi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                    That game hurt.   \n",
       "3                                 Man I love reddit.   \n",
       "5  Right? Considering itâ€™s such an important docu...   \n",
       "6  He isn't as big, but he's still quite popular....   \n",
       "7  That's crazy; I went to a super [RELIGION] hig...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                                          game hurt  \n",
       "3                                    man love reddit  \n",
       "5  right consider important document know damned ...  \n",
       "6  be not big he s still quite popular I ve hear ...  \n",
       "7  that s crazy go super religion high school thi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(\"preprocessing all reviews .....\")\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "print(\"Preprcessing Complete!\")\n",
    "print(\"\\nComparing Orignal vs Cleaned Text:\")\n",
    "display(df[['text','cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e480757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"cleaned_text\", \"sentiment\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5407037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"sentiment\"])\n",
    "train.to_csv(\"goemotions_train.csv\", index=False)\n",
    "test.to_csv(\"goemotions_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dca798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           cleaned_text sentiment\n",
      "0                                             game hurt  negative\n",
      "3                                       man love reddit  positive\n",
      "5     right consider important document know damned ...  positive\n",
      "6     be not big he s still quite popular I ve hear ...  negative\n",
      "7     that s crazy go super religion high school thi...  positive\n",
      "...                                                 ...       ...\n",
      "1009  probably overdose hotel lose virginity escort ...  positive\n",
      "1010  you re good troll good job way really get part...  positive\n",
      "1011                                          glad like  positive\n",
      "1012                                            gg haha  positive\n",
      "1013                    boobs penis quite embarrassment  negative\n",
      "\n",
      "[752 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "866e46a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PMLS\\OneDrive\\Desktop\\IMDB Semantic Analysis\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:909: UserWarning: unknown class(es) ['a', 'e', 'g', 'i', 'n', 'o', 'p', 's', 't', 'v'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Setup Tokenizer and Model\n",
    "model_name = \"prajjwal1/bert-tiny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "mlb = MultiLabelBinarizer(classes=emotion_cols)\n",
    "\n",
    "def preprocess(df):\n",
    "    enc = tokenizer(df[\"cleaned_text\"].tolist(), padding=True, truncation=True, max_length=128)\n",
    "    sentiment = mlb.fit_transform(df[\"sentiment\"])\n",
    "    return enc, sentiment\n",
    "\n",
    "train_enc, train_lbl = preprocess(train)\n",
    "test_enc, test_lbl = preprocess(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd1ee3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Build Dataset Class\n",
    "class EmoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, sentiment):\n",
    "        self.encodings = encodings\n",
    "        self.sentiment = sentiment\n",
    "    def __len__(self): return len(self.sentiment)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"sentiment\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "train_dataset = EmoDataset(train_enc, train_lbl)\n",
    "test_dataset = EmoDataset(test_enc, test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e898cba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "# Updated model initialization for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./emotions_out\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=False,\n",
    "    dataloader_num_workers=2\n",
    ")\n",
    "\n",
    "# Updated compute_metrics for binary classification\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1).numpy()\n",
    "    true = labels\n",
    "    micro = f1_score(true, preds, average=\"micro\")\n",
    "    macro = f1_score(true, preds, average=\"macro\")\n",
    "    return {\"micro_f1\": micro, \"macro_f1\": macro}\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb529ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PMLS\\OneDrive\\Desktop\\IMDB Semantic Analysis\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train and Evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab62bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Inference Example\n",
    "def predict(text):\n",
    "    enc = tokenizer([text], truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    outputs = model(**enc)\n",
    "    probs = torch.sigmoid(outputs.logits)[0].detach().numpy()\n",
    "    labels_pred = [emotion_cols[i] for i, p in enumerate(probs) if p >= 0.5]\n",
    "    return labels_pred\n",
    "\n",
    "print(predict(\"I really love your help, thanks so much!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85eba3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8db3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77169565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4df14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
